{"cells":[{"cell_type":"code","execution_count":null,"id":"86234fe2-a0ef-4128-9855-c5084c4d4fdf","metadata":{"id":"86234fe2-a0ef-4128-9855-c5084c4d4fdf","outputId":"6eef3e1a-7477-4d31-ba78-aa290bd173fb"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\rst20\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["!pip install --user -U nltk\n","\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import time\n","import warnings\n","\n","\n","from imblearn.metrics import classification_report_imbalanced\n","from imblearn.pipeline import make_pipeline, Pipeline\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","from sklearn.decomposition import TruncatedSVD, NMF\n","from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.naive_bayes import BernoulliNB, GaussianNB, ComplementNB, MultinomialNB\n","from sklearn.preprocessing import scale, StandardScaler, PolynomialFeatures, Normalizer\n","from sklearn.svm import LinearSVC\n","from sklearn.linear_model import SGDClassifier, LogisticRegression\n","\n","warnings.filterwarnings(\"ignore\")\n","nltk.download(\"stopwords\")\n"]},{"cell_type":"markdown","id":"26e9d900-cef3-4612-9eb4-ff75f8d4f456","metadata":{"id":"26e9d900-cef3-4612-9eb4-ff75f8d4f456"},"source":["Датасет был заранее скачан с кегла на гуглодиск"]},{"cell_type":"code","execution_count":null,"id":"7056704d-62fe-43ca-aba7-43d7cf12eb7c","metadata":{"id":"7056704d-62fe-43ca-aba7-43d7cf12eb7c","outputId":"e7ec34d9-7020-4e2f-e145-8c42abd1dff7"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  label                                              tweet\n","0   1      0   @user when a father is dysfunctional and is s...\n","1   2      0  @user @user thanks for #lyft credit i can't us...\n","2   3      0                                bihday your majesty\n","3   4      0  #model   i love u take with u all the time in ...\n","4   5      0             factsguide: society now    #motivation"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.read_csv('https://drive.google.com/uc?id=1_tZHuLfFz5_xUau-Pr-1grmzSkEYnleb', sep=',')\n","df_test = pd.read_csv('https://drive.google.com/uc?id=11Z11g5fwGgAsrasmV-AC5Yi0_DGdetA7', sep=',')\n","df_train.head()"]},{"cell_type":"markdown","id":"2d8f173b-2ea2-4f12-a802-b0405d3d87f6","metadata":{"id":"2d8f173b-2ea2-4f12-a802-b0405d3d87f6"},"source":["Так как даная задача связана с анализом текстов (что и подтверждает датасет), то обычные шаги по предварительному анализу и очистке датасета представляются излишними: векторизация представит данные в виде сотен тысяч признаков, которые ничего не скажут человеческому глазу, а пытаться выделить выбросы автоматическими методами, основанными на поиске ближайших соседей --- крайне затратное с точки зрения времени работы мероприятие.\n","\n","В описании датасета утверждается, что он размечен на две категории"]},{"cell_type":"code","execution_count":null,"id":"3f02c52e-cc07-47c3-9acc-20f424eb0c84","metadata":{"id":"3f02c52e-cc07-47c3-9acc-20f424eb0c84","outputId":"21f8e03b-5181-44f4-8549-e139f039cf77"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 31962 entries, 0 to 31961\n","Data columns (total 3 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   id      31962 non-null  int64 \n"," 1   label   31962 non-null  int64 \n"," 2   tweet   31962 non-null  object\n","dtypes: int64(2), object(1)\n","memory usage: 749.2+ KB\n","None\n","label\n","0    29720\n","1     2242\n","Name: count, dtype: int64\n"]}],"source":["print(df_train.info())\n","print(df_train['label'].value_counts())"]},{"cell_type":"markdown","id":"3737f961-82ad-4d41-9817-844650b9cbad","metadata":{"id":"3737f961-82ad-4d41-9817-844650b9cbad"},"source":["Распределение классов несбалансировано: сильно смещено в сторону класса 0."]},{"cell_type":"code","execution_count":null,"id":"91be0f2d-603d-44b7-9b56-2c83de74ef9d","metadata":{"id":"91be0f2d-603d-44b7-9b56-2c83de74ef9d","outputId":"23f3dc2c-1c2c-4423-9b42-4be09bcb76c5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>31963</td>\n","      <td>#studiolife #aislife #requires #passion #dedic...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>31964</td>\n","      <td>@user #white #supremacists want everyone to s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>31965</td>\n","      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>31966</td>\n","      <td>is the hp and the cursed child book up for res...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>31967</td>\n","      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id                                              tweet\n","0  31963  #studiolife #aislife #requires #passion #dedic...\n","1  31964   @user #white #supremacists want everyone to s...\n","2  31965  safe ways to heal your #acne!!    #altwaystohe...\n","3  31966  is the hp and the cursed child book up for res...\n","4  31967    3rd #bihday to my amazing, hilarious #nephew..."]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df_test.head()"]},{"cell_type":"markdown","id":"bf9faf7a-5807-4a48-9eb1-f995dca22a9c","metadata":{"id":"bf9faf7a-5807-4a48-9eb1-f995dca22a9c"},"source":["Данные во втором файле, названном авторами датасета тестовыми, не содержат колонку label, поэтому мы их с негодованием отвергаем, и в дальнейшем речи о них здесь не будет."]},{"cell_type":"markdown","id":"2bc9f068-fdc8-4453-a893-83573d9abac5","metadata":{"id":"2bc9f068-fdc8-4453-a893-83573d9abac5"},"source":["Для того, чтобы хоть как-то уменьшить объём работы алгоритмам, загрузим стоп-слова на английском: в дельнейшем будем передавать их векторизаторам."]},{"cell_type":"code","execution_count":null,"id":"73851b83-e16e-4d60-9c84-9478bf7cee5b","metadata":{"id":"73851b83-e16e-4d60-9c84-9478bf7cee5b","outputId":"5889520b-a4cc-4579-dc6a-39a39e4eeac5"},"outputs":[{"data":{"text/plain":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","stop_words[:10]"]},{"cell_type":"markdown","id":"70718134-a174-4e0b-8054-7d759181e1fb","metadata":{"id":"70718134-a174-4e0b-8054-7d759181e1fb"},"source":["Классический подход: Bag of words"]},{"cell_type":"code","execution_count":null,"id":"19a6969f-4647-43fb-a51c-da4b4c8f3709","metadata":{"id":"19a6969f-4647-43fb-a51c-da4b4c8f3709","outputId":"45be65b8-7f37-4681-c910-db434407b583"},"outputs":[{"name":"stdout","output_type":"stream","text":["размер словаря: 374556 размерность данных: (31962, 374556)\n","векторизованное значение: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"]}],"source":["vectorizer = CountVectorizer(stop_words=stop_words, ngram_range=(1, 3)).fit(df_train['tweet'])\n","\n","X_all_data = vectorizer.transform(df_train['tweet'])\n","Y_all_data = df_train['label']\n","print('размер словаря:', len(vectorizer.vocabulary_), 'размерность данных:', X_all_data.shape)\n","elem = X_all_data[0]\n","print('векторизованное значение:', elem[elem.nonzero()])\n","\n","# работает очень долго\n","#projector = NMF(n_components=100)\n","#X_truncated = projector.fit_transform(X_all_data)\n","#print('размерность данных:', X_truncated.shape)\n","#print(X_truncated[0])\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_all_data, Y_all_data, test_size=0.2, shuffle=True)"]},{"cell_type":"markdown","id":"69bc50f7-bf82-43b9-8415-ecb310367abf","metadata":{"id":"69bc50f7-bf82-43b9-8415-ecb310367abf"},"source":["Простейшая проверка работы алгоритмов:"]},{"cell_type":"code","execution_count":null,"id":"b06372b7-24ab-4dbb-9cb4-4984a281b586","metadata":{"id":"b06372b7-24ab-4dbb-9cb4-4984a281b586","outputId":"60bbb67c-0c5e-44c3-d63c-2d8a4117d800"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.98      0.90      0.94      5994\n","           1       0.34      0.77      0.47       399\n","\n","    accuracy                           0.89      6393\n","   macro avg       0.66      0.84      0.71      6393\n","weighted avg       0.94      0.89      0.91      6393\n","\n"]}],"source":["clf = MultinomialNB()\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","print(classification_report(y_test, y_pred))\n"]},{"cell_type":"markdown","id":"f81e67bc-2016-4781-ab50-3eb3fb17c4db","metadata":{"id":"f81e67bc-2016-4781-ab50-3eb3fb17c4db"},"source":["В предыдущих заданиях масштабирование данных помогало улучшить точность предсказаний. Попробуем аналогичный подход для наших данных и наших алгоритмов:"]},{"cell_type":"code","execution_count":null,"id":"6dc78076-efcf-436e-9563-ac8cae3f10a2","metadata":{"id":"6dc78076-efcf-436e-9563-ac8cae3f10a2","outputId":"9c1570cd-405a-4f49-9179-a7afd7301c5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+ масштабирование: [[0.24314167 0.16896196 0.04678636 0.24314167 0.24314167 0.20052484\n","  0.15407348 0.24314167 0.24314167 0.13898553 0.24314167 0.24314167\n","  0.11550985 0.24314167 0.24314167 0.24314167 0.24314167 0.23391012\n","  0.24314167 0.24314167 0.22227974]]\n","              precision    recall  f1-score   support\n","\n","           0       0.94      1.00      0.97      5947\n","           1       1.00      0.10      0.18       446\n","\n","    accuracy                           0.94      6393\n","   macro avg       0.97      0.55      0.57      6393\n","weighted avg       0.94      0.94      0.91      6393\n","\n"]}],"source":["X_norm = Normalizer().fit_transform(X_all_data)\n","elem = X_norm[0]\n","print('+ масштабирование:', elem[elem.nonzero()])\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_norm, Y_all_data, test_size=0.2, shuffle=True)\n","clf = MultinomialNB()\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","id":"2dbddad4-efe2-4043-8035-257c53ebeb5c","metadata":{"id":"2dbddad4-efe2-4043-8035-257c53ebeb5c"},"source":["Неудачный опыт.\n","\n","Попробуем сбалансировать данные.\n","Количество признаков для наших данных после векторизации довольно велико, поэтому оверсемплинг представляется не самой удачной идеей.\n","А вот уменьшить количество экземпляров класса большей мощности можно достаточно легко (в вычислительном смысле)"]},{"cell_type":"code","execution_count":null,"id":"8d510b17-c0e3-4198-8561-4096ebe2461a","metadata":{"id":"8d510b17-c0e3-4198-8561-4096ebe2461a","outputId":"c31fc83c-2e0a-4179-cb89-0358e3eeed76"},"outputs":[{"name":"stdout","output_type":"stream","text":["исходное распределение по классам: (array([0, 1], dtype=int64), array([23773,  1796], dtype=int64))\n","скорректированное распределение по классам: (array([0, 1], dtype=int64), array([7184, 1796], dtype=int64))\n"]}],"source":["print('исходное распределение по классам:', np.unique(y_train, return_counts=True))\n","sampler = RandomUnderSampler(sampling_strategy=0.25)\n","# параметр sampling_strategy у андерсэмплера достоин подбора\n","X_train_balanced, y_train_balanced = sampler.fit_resample(X_train, y_train)\n","print('скорректированное распределение по классам:', np.unique(y_train_balanced, return_counts=True))"]},{"cell_type":"markdown","id":"93ab9f81-e0c5-4a31-9774-474bc25f447f","metadata":{"id":"93ab9f81-e0c5-4a31-9774-474bc25f447f"},"source":["Сравним различные варианты алгоритмов на основе \"наивного\" применения теоремы Байеса:"]},{"cell_type":"code","execution_count":null,"id":"91be5747-09ca-4d01-b37d-588248ee0994","metadata":{"id":"91be5747-09ca-4d01-b37d-588248ee0994","outputId":"676d0fdc-6648-4c24-d30b-e21e349a2b78"},"outputs":[{"name":"stdout","output_type":"stream","text":["MultinomialNB\n","              precision    recall  f1-score   support\n","\n","           0       0.94      1.00      0.97      5947\n","           1       1.00      0.18      0.30       446\n","\n","    accuracy                           0.94      6393\n","   macro avg       0.97      0.59      0.64      6393\n","weighted avg       0.95      0.94      0.92      6393\n","\n","ComplementNB\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.97      0.97      5947\n","           1       0.62      0.70      0.66       446\n","\n","    accuracy                           0.95      6393\n","   macro avg       0.80      0.83      0.82      6393\n","weighted avg       0.95      0.95      0.95      6393\n","\n","BernoulliNB\n","              precision    recall  f1-score   support\n","\n","           0       0.93      1.00      0.96      5947\n","           1       0.00      0.00      0.00       446\n","\n","    accuracy                           0.93      6393\n","   macro avg       0.47      0.50      0.48      6393\n","weighted avg       0.87      0.93      0.90      6393\n","\n","параметры по умолчанию дают несколько странные результаты\n","BernoulliNB\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.80      0.88      5947\n","           1       0.24      0.84      0.37       446\n","\n","    accuracy                           0.80      6393\n","   macro avg       0.61      0.82      0.62      6393\n","weighted avg       0.93      0.80      0.84      6393\n","\n"]}],"source":["def check_classifier(clf):\n","    print(type(clf).__name__)\n","    clf.fit(X_train_balanced, y_train_balanced)\n","    y_pred = clf.predict(X_test)\n","    print(classification_report(y_test, y_pred))\n","\n","check_classifier(MultinomialNB())\n","check_classifier(ComplementNB())\n","check_classifier(BernoulliNB())\n","print('параметры по умолчанию дают несколько странные результаты')\n","check_classifier(BernoulliNB(alpha=0.1))\n","# параметр alpha у классификатора достоин подбора"]},{"cell_type":"markdown","id":"f694e6cd-4392-4b5d-bca2-a91b2010bb50","metadata":{"id":"f694e6cd-4392-4b5d-bca2-a91b2010bb50"},"source":["С учётом экспериментов выше, попробуем подобрать параметры:"]},{"cell_type":"code","execution_count":null,"id":"eded06ce-407c-4bd4-82f4-dbd37129aeab","metadata":{"scrolled":true,"id":"eded06ce-407c-4bd4-82f4-dbd37129aeab","outputId":"3ab8f587-88d9-4a1f-b1ef-5579aaa91269"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n","подбор параметров: 251.31895650000024 с\n","{'classifier__estimator': MultinomialNB(force_alpha=True), 'classifier__estimator__alpha': 0.011111111, 'undersampler__sampling_strategy': 0.4} Pipeline(steps=[('undersampler', RandomUnderSampler(sampling_strategy=0.4)),\n","                ('classifier',\n","                 AdaBoostClassifier(estimator=MultinomialNB(alpha=0.011111111,\n","                                                            force_alpha=True),\n","                                    n_estimators=20))])\n","                   pre       rec       spe        f1       geo       iba       sup\n","\n","          0       0.95      0.99      0.41      0.97      0.63      0.43      5924\n","          1       0.74      0.41      0.99      0.52      0.63      0.38       469\n","\n","avg / total       0.94      0.95      0.45      0.94      0.63      0.42      6393\n","\n"]}],"source":["model = Pipeline(steps=[\n","    ('undersampler', RandomUnderSampler()),\n","    ('classifier', AdaBoostClassifier(n_estimators=20))\n","])\n","params_grid = {\n","    'undersampler__sampling_strategy': np.array(np.linspace(0.1, 1, 10), dtype='float32'),\n","    'classifier__estimator': [MultinomialNB(force_alpha=True), ComplementNB(force_alpha=True), BernoulliNB(force_alpha=True)],\n","    'classifier__estimator__alpha':  np.array(np.linspace(0, 0.1, 10), dtype='float32'),\n","}\n","start = time.perf_counter()\n","gs = GridSearchCV(model, params_grid, cv=5, n_jobs=-1, verbose=3)\n","gs.fit(X_train, y_train)\n","print('подбор параметров:', time.perf_counter() - start, 'с')\n","\n","print(gs.best_params_, gs.best_estimator_)\n","print(classification_report_imbalanced(y_test, gs.predict(X_test)))\n"]},{"cell_type":"markdown","id":"065a12da-b029-454c-b41c-301d55f8422c","metadata":{"id":"065a12da-b029-454c-b41c-301d55f8422c"},"source":["Попытка улучшить алгоритм с помощью адаптивного бустинга дала не особо хорошие результаты. Параметр f1 для лучшего из вариантов хуже, чем лучший из опробованных выше алгоритмов с параметрами по умолчанию.\n","Попробуем ещё раз подобрать парамтеры, но теперь упакуем классификаторы в BaggingClassifier:"]},{"cell_type":"code","execution_count":null,"id":"4fe83d83-bc3a-44c1-b333-3cbef2032f91","metadata":{"id":"4fe83d83-bc3a-44c1-b333-3cbef2032f91","outputId":"35d21a1e-7fd5-4e40-f885-bb6b331bca09"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n","подбор параметров: 126.2759087000004 с\n","{'classifier__estimator': MultinomialNB(force_alpha=True), 'classifier__estimator__alpha': 0.1, 'undersampler__sampling_strategy': 0.3} Pipeline(steps=[('undersampler', RandomUnderSampler(sampling_strategy=0.3)),\n","                ('classifier',\n","                 BaggingClassifier(estimator=MultinomialNB(alpha=0.1,\n","                                                           force_alpha=True)))])\n","                   pre       rec       spe        f1       geo       iba       sup\n","\n","          0       0.97      0.99      0.63      0.98      0.79      0.65      5924\n","          1       0.81      0.63      0.99      0.71      0.79      0.60       469\n","\n","avg / total       0.96      0.96      0.66      0.96      0.79      0.64      6393\n","\n"]}],"source":["model = Pipeline(steps=[\n","    ('undersampler', RandomUnderSampler()),\n","    ('classifier', BaggingClassifier())\n","])\n","params_grid = {\n","    'undersampler__sampling_strategy': np.array(np.linspace(0.1, 1, 10), dtype='float32'),\n","    'classifier__estimator': [MultinomialNB(force_alpha=True), ComplementNB(force_alpha=True), BernoulliNB(force_alpha=True)],\n","    'classifier__estimator__alpha':  np.array(np.linspace(0, 0.1, 10), dtype='float32'),\n","}\n","start = time.perf_counter()\n","gs = GridSearchCV(model, params_grid, cv=5, n_jobs=-1, verbose=3)\n","gs.fit(X_train, y_train)\n","print('подбор параметров:', time.perf_counter() - start, 'с')\n","\n","print(gs.best_params_, gs.best_estimator_)\n","print(classification_report_imbalanced(y_test, gs.predict(X_test)))\n"]},{"cell_type":"markdown","id":"34bb95b9-ba57-442b-9df4-b6a2691d6633","metadata":{"id":"34bb95b9-ba57-442b-9df4-b6a2691d6633"},"source":["Всё стало заметно лучше. И к тому же, за меньшее время."]},{"cell_type":"markdown","id":"3add8a28-db48-4a03-81b1-5a3dde2844d5","metadata":{"id":"3add8a28-db48-4a03-81b1-5a3dde2844d5"},"source":["Попробуем обучить алгоритм \"с помощью классических от TruncatedSVD на основе полиномиальных от tf-idf с (1-3) биграммами\".\n","\n","Так как TruncatedSVD возвращает признаки, которые не являются неотрицательными, то с этим инструментом понижения размерности не получится использовать алгоритмы на основе наивного Байеса: но зато есть множество других алгоритмов."]},{"cell_type":"code","execution_count":null,"id":"604ab289-5190-43c5-be1b-ffa4b42baa4c","metadata":{"id":"604ab289-5190-43c5-be1b-ffa4b42baa4c","outputId":"5b13a52b-028a-4d36-aedd-8e6410b01cf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["уменьшение размерности: 9.355633600000147 с\n","масштабирование: 0.04042459999982384 с\n","андерсемплинг: 0.007768800000121701 с\n","LinearSVC\n","обучение классификатора: 3.949679199999082 с\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.98      0.97      5962\n","           1       0.53      0.34      0.41       431\n","\n","    accuracy                           0.94      6393\n","   macro avg       0.74      0.66      0.69      6393\n","weighted avg       0.92      0.94      0.93      6393\n","\n","SGDClassifier\n","обучение классификатора: 0.5941450000009354 с\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.97      0.96      5962\n","           1       0.46      0.39      0.43       431\n","\n","    accuracy                           0.93      6393\n","   macro avg       0.71      0.68      0.69      6393\n","weighted avg       0.92      0.93      0.93      6393\n","\n","LogisticRegression\n","обучение классификатора: 0.7008351999993465 с\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.97      0.96      5962\n","           1       0.49      0.40      0.44       431\n","\n","    accuracy                           0.93      6393\n","   macro avg       0.72      0.68      0.70      6393\n","weighted avg       0.93      0.93      0.93      6393\n","\n","RandomForestClassifier\n","обучение классификатора: 66.47231450000072 с\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.98      0.97      5962\n","           1       0.63      0.42      0.51       431\n","\n","    accuracy                           0.94      6393\n","   macro avg       0.79      0.70      0.74      6393\n","weighted avg       0.94      0.94      0.94      6393\n","\n","RandomForestClassifier\n","обучение классификатора: 6.573983300000691 с\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.98      0.97      5962\n","           1       0.59      0.41      0.49       431\n","\n","    accuracy                           0.94      6393\n","   macro avg       0.77      0.70      0.73      6393\n","weighted avg       0.93      0.94      0.94      6393\n","\n"]}],"source":["vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 3)).fit(df_train['tweet'])\n","X_all_data = vectorizer.transform(df_train['tweet'])\n","Y_all_data = df_train['label']\n","\n","projector = TruncatedSVD(n_components=100, algorithm='randomized')\n","start = time.perf_counter()\n","X_truncated = projector.fit_transform(X_all_data)\n","print('уменьшение размерности:', time.perf_counter() - start, 'с')\n","\n","scaler = StandardScaler()\n","start = time.perf_counter()\n","X_scaled = scaler.fit_transform(X_truncated)\n","print('масштабирование:', time.perf_counter() - start, 'с')\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y_all_data, test_size=0.2, shuffle=True)\n","\n","sampler = RandomUnderSampler(sampling_strategy=0.25)\n","start = time.perf_counter()\n","X_train_balanced, y_train_balanced = sampler.fit_resample(X_train, y_train)\n","print('андерсемплинг:', time.perf_counter() - start, 'с')\n","\n","def check_classifier(clf):\n","    print(type(clf).__name__)\n","    b_clf =BaggingClassifier(estimator=clf)\n","    start = time.perf_counter()\n","    b_clf.fit(X_train_balanced, y_train_balanced)\n","    print('обучение классификатора:', time.perf_counter() - start, 'с')\n","    y_pred = b_clf.predict(X_test)\n","    print(classification_report(y_test, y_pred))\n","\n","check_classifier(LinearSVC(dual=False))\n","check_classifier(SGDClassifier())\n","check_classifier(LogisticRegression())\n","check_classifier(RandomForestClassifier())\n","check_classifier(RandomForestClassifier(n_estimators=10))\n"]},{"cell_type":"markdown","id":"453f2bda-cf60-467f-841d-0fba2472bbb3","metadata":{"id":"453f2bda-cf60-467f-841d-0fba2472bbb3"},"source":["Прямо скажем, точность результатов не впечатляет. Возможно, подбором параметров удастся повысить качество работы алгоритма, но чтобы превзойти результаты, полученные наивным байесом придётся перебирать очень большое пространство параметров без какой-либо гарантии успеха.\n","\n","Итого: лучшие результаты удалось получить следующим алгоритмом:\n","\n","{\n","\n","'classifier__estimator': MultinomialNB(force_alpha=True),\n","\n","'classifier__estimator__alpha': 0.1,\n","\n","'undersampler__sampling_strategy': 0.3\n","\n","}\n","\n","Pipeline(steps=\\[\n","\t('undersampler', RandomUnderSampler(sampling_strategy=0.3)),\n","\t('classifier', BaggingClassifier(estimator=MultinomialNB(alpha=0.1, force_alpha=True)))\n","\t\\])"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}